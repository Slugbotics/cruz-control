{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies \n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip3 install nuscenes-devkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.can_bus.can_bus_api import NuScenesCanBus\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "PATH = '/Users/jonathanmorris/Downloads/v1.0-mini'\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=PATH, verbose=True)\n",
    "nusc_can = NuScenesCanBus(dataroot=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.list_scenes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting first scene\n",
    "my_scene = nusc.scene[3]\n",
    "my_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = nusc.get('log', my_scene['log_token'])\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data from can expansion\n",
    "my_scene_zoe_can = nusc_can.get_messages(my_scene['name'], 'zoesensors')\n",
    "my_scene_zoe_can[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scene_imu_can = nusc_can.get_messages(my_scene['name'], 'ms_imu')\n",
    "my_scene_imu_can[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scene_vehicle_monitor_can = nusc_can.get_messages(my_scene['name'], 'vehicle_monitor')\n",
    "my_scene_vehicle_monitor_can[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first sample\n",
    "first_sample_token = my_scene['first_sample_token']\n",
    "my_sample = nusc.get('sample', first_sample_token)\n",
    "my_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nusc_can.can_blacklist)\n",
    "print(nusc.scene[0]['name'])\n",
    "print(int(nusc.scene[0]['name'].split(\"-\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_can(time, can_objects):\n",
    "    closest = {}\n",
    "    prev_diff = 1000000 # 1 Second in microseconds\n",
    "    for object in can_objects:\n",
    "        diff = object[\"utime\"] - time\n",
    "        if diff > 0 and diff < prev_diff:\n",
    "            closest = object\n",
    "            prev_diff = diff\n",
    "    # print(\"Time difference: \", prev_diff)\n",
    "    return closest\n",
    "\n",
    "def normalize(value, min, max):\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = max - min\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    return float(value - min) / float(leftSpan)\n",
    "\n",
    "def normalize_zoe_can(can_obj):\n",
    "    new_obj = {}\n",
    "    \n",
    "    # These values are from here: https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/can_bus/README.md#zoe-sensors\n",
    "    min_breaking = 0.166\n",
    "    max_breaking = 0.631\n",
    "\n",
    "    min_steering = 0.176\n",
    "    max_steering = 0.252\n",
    "    \n",
    "    min_throttle = 0.105\n",
    "    max_throttle = 0.411\n",
    "\n",
    "    # convert values from 0-1\n",
    "    new_obj[\"brake_sensor\"] = normalize(can_obj[\"brake_sensor\"], min_breaking, max_breaking)\n",
    "    new_obj[\"steering_sensor\"] = normalize(can_obj[\"steering_sensor\"], min_steering, max_steering)\n",
    "    new_obj[\"throttle_sensor\"] = normalize(can_obj[\"throttle_sensor\"], min_throttle, max_throttle)\n",
    "\n",
    "    # reduce precision\n",
    "    new_obj[\"brake_sensor\"] = round(can_obj[\"brake_sensor\"], 2)\n",
    "    new_obj[\"steering_sensor\"] = round(can_obj[\"steering_sensor\"], 2)\n",
    "    new_obj[\"throttle_sensor\"] = round(can_obj[\"throttle_sensor\"], 2)\n",
    "    \n",
    "    return new_obj\n",
    "\n",
    "def normalize_vehicle_monitor_can(can_obj):\n",
    "    new_obj = {}\n",
    "\n",
    "    min_brake = 0\n",
    "    max_break = 126\n",
    "\n",
    "    min_steering = -780\n",
    "    max_steering = 779.9\n",
    "\n",
    "    min_throttle = 0\n",
    "    max_throttle = 1000\n",
    "\n",
    "    new_obj[\"brake\"] = normalize(can_obj[\"brake\"], min_brake, max_break)\n",
    "    new_obj[\"steering\"] = normalize(can_obj[\"steering\"], min_steering, max_steering)\n",
    "    new_obj[\"throttle\"] = normalize(can_obj[\"throttle\"], min_throttle, max_throttle)\n",
    "\n",
    "    return new_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_data = get_closest_can(my_sample['timestamp'], my_scene_zoe_can)\n",
    "can_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_zoe_can(can_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_data = get_closest_can(my_sample['timestamp'], my_scene_imu_can)\n",
    "imu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceleration on x, y, z axis, m/s^2\n",
    "acc = imu_data['linear_accel']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = torch.tensor([acc[0], acc[1], acc[2]])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tensor size\n",
    "acc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = 'CAM_FRONT'\n",
    "cam_front_data = nusc.get('sample_data', my_sample['data'][sensor])\n",
    "current_image_path = PATH+\"/\"+cam_front_data['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(current_image_path)\n",
    "convert_tensor = transforms.ToTensor()\n",
    "tensor = convert_tensor(img)\n",
    "tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_data(scene, sample):\n",
    "    next_token = sample[\"next\"]\n",
    "    next_sample = nusc.get(\"sample\", next_token)\n",
    "    can_data = nusc_can.get_messages(scene['name'], 'zoesensors')\n",
    "    get_closest_can(next_sample[\"timestamp\"], my_scene_zoe_can)\n",
    "    return (next_sample, can_data)\n",
    "\n",
    "get_next_data(my_scene,my_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaneCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 28 * 28, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 3) # output three values -> [steering, throttle, breaking]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = x.view(256 * 28 * 28)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        # Image processing branch - Convolutional layers\n",
    "        self.image_branch = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),  # Adjusted input size\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Float processing branch - Fully connected layers\n",
    "        self.float_branch = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Final output layer\n",
    "        self.final_layer = nn.Linear(128 + 32, 3)\n",
    "\n",
    "    def forward(self, image, floats):\n",
    "        image_out = self.image_branch(image)\n",
    "        float_out = self.float_branch(floats)\n",
    "        float_out = float_out.unsqueeze(0).repeat(image_out.size(0), 1)\n",
    "\n",
    "        # Concatenate the outputs of the two branches\n",
    "        concat_out = torch.cat((image_out, float_out), dim=1)\n",
    "        \n",
    "        # Pass through final layer\n",
    "        out = self.final_layer(concat_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "model_path = os.path.join(os.getcwd(), \"models\")\n",
    "!rm -rf models\n",
    "!mkdir models && cd models && mkdir epochs\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # path = sys.argv[1]\n",
    "    print(\"Model Version V1.1\")\n",
    "    print(\"final model weights will be saved to: \" + model_path)\n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224), antialias=True), transforms.ToTensor()]\n",
    "    )\n",
    "\n",
    "    scenes = nusc.scene\n",
    "\n",
    "    train_size = int(0.8 * len(scenes))\n",
    "    val_size = len(scenes) - train_size\n",
    "    train, val = random_split(scenes, [train_size, val_size])\n",
    "\n",
    "    # multithreaded data loading\n",
    "    trainloader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    valloader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    net = LaneCNN().to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        for scene in trainloader:\n",
    "            for scene in scenes:\n",
    "                scene_number = int(scene['name'].split(\"-\")[1])\n",
    "\n",
    "                if scene_number in nusc_can.can_blacklist:\n",
    "                    print(\"Skipping scene \" + str(scene_number))\n",
    "                    continue\n",
    "                \n",
    "                first_sample_token = scene['first_sample_token']\n",
    "\n",
    "                current_sample = nusc.get('sample', first_sample_token)\n",
    "\n",
    "                scene_vehicle_monitor = nusc_can.get_messages(scene['name'], 'vehicle_monitor')\n",
    "\n",
    "                while True:\n",
    "                    sensor = \"CAM_FRONT\"\n",
    "                    cam_front_data = nusc.get(\"sample_data\", current_sample[\"data\"][sensor])\n",
    "                    current_image_path = PATH + \"/\" + cam_front_data[\"filename\"]\n",
    "                    img = Image.open(current_image_path)\n",
    "\n",
    "                    img_input = transform(img).to(device)\n",
    "                \n",
    "                    current_vehicle_can = get_closest_can(current_sample[\"timestamp\"], scene_vehicle_monitor)                    \n",
    "\n",
    "                    if current_vehicle_can == {}:\n",
    "                        if current_sample['next'] == '':\n",
    "                            break\n",
    "                        else:\n",
    "                            current_sample = nusc.get('sample', current_sample['next'])\n",
    "\n",
    "                    normal_vm_can = normalize_vehicle_monitor_can(current_vehicle_can)\n",
    "\n",
    "                    steering_targets = normal_vm_can['steering']\n",
    "                    throttle_targets = normal_vm_can['throttle']\n",
    "                    breaking_targets = normal_vm_can['brake']\n",
    "\n",
    "                    label = torch.FloatTensor([steering_targets, throttle_targets, breaking_targets]).to(device)              \n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = net(img_input)\n",
    "\n",
    "                    # Compute loss\n",
    "                    total_loss = criterion(outputs, label)\n",
    "\n",
    "                    # Backward pass\n",
    "                    total_loss.backward()\n",
    "\n",
    "                    # Update weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if current_sample['next'] == '':\n",
    "                        break\n",
    "                    else:\n",
    "                        current_sample = nusc.get('sample', current_sample['next'])\n",
    "\n",
    "        # Validation\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for scene in valloader:\n",
    "                for scene in scenes:\n",
    "                    scene_number = int(scene['name'].split(\"-\")[1])\n",
    "\n",
    "                    if scene_number in nusc_can.can_blacklist:\n",
    "                        print(\"Skipping scene \" + str(scene_number))\n",
    "                        continue\n",
    "\n",
    "                    first_sample_token = scene['first_sample_token']\n",
    "\n",
    "                    current_sample = nusc.get('sample', first_sample_token)\n",
    "\n",
    "                    scene_vehicle_monitor = nusc_can.get_messages(scene['name'], 'vehicle_monitor')\n",
    "                    scene_imu_cans = nusc_can.get_messages(scene['name'], 'ms_imu')\n",
    "\n",
    "                    while True:\n",
    "                        sensor = \"CAM_FRONT\"\n",
    "                        cam_front_data = nusc.get(\"sample_data\", current_sample[\"data\"][sensor])\n",
    "                        current_image_path = PATH + \"/\" + cam_front_data[\"filename\"]\n",
    "                        img = Image.open(current_image_path)\n",
    "\n",
    "                        img_input = transform(img).to(device)\n",
    "\n",
    "                        current_vehicle_can = get_closest_can(current_sample[\"timestamp\"], scene_vehicle_monitor)\n",
    "\n",
    "                        if current_vehicle_can == {}:\n",
    "                            if current_sample['next'] == '':\n",
    "                                break\n",
    "                            else:\n",
    "                                current_sample = nusc.get('sample', current_sample['next'])\n",
    "                        \n",
    "                        normal_vm_can = normalize_vehicle_monitor_can(current_vehicle_can)\n",
    "\n",
    "                        steering_targets = normal_vm_can['steering']\n",
    "                        throttle_targets = normal_vm_can['throttle']\n",
    "                        breaking_targets = normal_vm_can['brake']\n",
    "\n",
    "                        label = torch.FloatTensor([steering_targets, throttle_targets, breaking_targets]).to(device)\n",
    "\n",
    "                        outputs = net(img_input)\n",
    "\n",
    "                        val_total_loss = criterion(outputs, label)\n",
    "\n",
    "                        if current_sample['next'] == '':\n",
    "                            break\n",
    "                        else:\n",
    "                            current_sample = nusc.get('sample', current_sample['next'])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}, Validation Loss: {val_total_loss.item():.4f}\"\n",
    "        )\n",
    "        torch.save(\n",
    "            net.state_dict(),\n",
    "            os.path.join(model_path, \"epochs\", f\"model_e{epoch+1}.pth\"),\n",
    "        )\n",
    "\n",
    "    print(\"Finished training\")\n",
    "    torch.save(net.state_dict(), os.path.join(model_path, f\"model_final.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
