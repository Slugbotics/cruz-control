{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies \n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip3 install nuscenes-devkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Image as IPImage\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.739 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from nuscenes.can_bus.can_bus_api import NuScenesCanBus\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "PATH = '/Users/jonathanmorris/Downloads/v1.0-mini'\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=PATH, verbose=True)\n",
    "nusc_can = NuScenesCanBus(dataroot=PATH)\n",
    "HEIGHT = 70\n",
    "WIDTH = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_can(time, can_objects):\n",
    "    closest = {}\n",
    "    prev_diff = 1000000 # 1 Second in microseconds\n",
    "    for object in can_objects:\n",
    "        diff = object[\"utime\"] - time\n",
    "        if diff > 0 and diff < prev_diff:\n",
    "            closest = object\n",
    "            prev_diff = diff\n",
    "    # print(\"Time difference: \", prev_diff)\n",
    "    return closest\n",
    "\n",
    "def num_to_range(num, inMin, inMax, outMin, outMax):\n",
    "    return outMin + (float(num - inMin) / float(inMax - inMin) * (outMax - outMin))\n",
    "\n",
    "def normalize_vehicle_monitor_can(can_obj):\n",
    "    new_obj = {}\n",
    "\n",
    "    min_brake = 0\n",
    "    max_break = 126\n",
    "\n",
    "    min_steering = -780\n",
    "    max_steering = 779.9\n",
    "\n",
    "    min_throttle = 0\n",
    "    max_throttle = 1000\n",
    "\n",
    "    new_obj[\"brake\"] = num_to_range(can_obj[\"brake\"], min_brake, max_break, 0, 1)\n",
    "    new_obj[\"steering\"] = num_to_range(can_obj[\"steering\"], min_steering, max_steering, -1, 1)\n",
    "    new_obj[\"throttle\"] = num_to_range(can_obj[\"throttle\"], min_throttle, max_throttle, 0, 1)\n",
    "\n",
    "    new_obj[\"brake\"] = round(new_obj[\"brake\"], 1)\n",
    "    new_obj[\"steering\"] = round(new_obj[\"steering\"], 1)\n",
    "    new_obj[\"throttle\"] = round(new_obj[\"throttle\"], 1)\n",
    "\n",
    "    return new_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuScenesImageDataset(Dataset):\n",
    "    \"\"\"The training table dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, nusc, nusc_can, transform=None):\n",
    "        self.nusc = nusc\n",
    "        self.nusc_can = nusc_can\n",
    "        self.transform = transform\n",
    "        self.x_image_data = []\n",
    "        self.y_vehicle_data = []\n",
    "        self.len = len(nusc.scene)\n",
    "\n",
    "        for scene in nusc.scene:\n",
    "            scene_token = scene['token']\n",
    "            scene_record = nusc.get('scene', scene_token)\n",
    "            sample_token = scene_record['first_sample_token']\n",
    "            while sample_token != '':\n",
    "                sample_record = nusc.get('sample', sample_token)\n",
    "                sample_data = nusc.get('sample_data', sample_record['data'][\"CAM_FRONT\"])\n",
    "                image_path = nusc.get_sample_data_path(sample_data['token'])\n",
    "                image = Image.open(image_path)\n",
    "                image = image.resize((HEIGHT, WIDTH))\n",
    "                self.x_image_data.append(image)\n",
    "                can_objects = nusc_can.get_messages(sample_data['timestamp'], sample_data['timestamp'] + 1000000)\n",
    "                closest_can = get_closest_can(sample_data['timestamp'], can_objects)\n",
    "                vehicle_data = normalize_vehicle_monitor_can(closest_can)\n",
    "                self.y_vehicle_data.append(vehicle_data)\n",
    "                sample_token = sample_record['next']\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Zhenye-Na/e2e-learning-self-driving-cars/blob/master/src/train.ipynb\n",
    "\n",
    "class NetworkNvidia(nn.Module):\n",
    "    \"\"\"NVIDIA model used in the paper.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize NVIDIA model.\n",
    "\n",
    "        NVIDIA model used\n",
    "            Image normalization to avoid saturation and make gradients work better.\n",
    "            Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n",
    "            Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n",
    "            Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n",
    "            Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "            Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "            Drop out (0.5)\n",
    "            Fully connected: neurons: 100, activation: ELU\n",
    "            Fully connected: neurons: 50, activation: ELU\n",
    "            Fully connected: neurons: 10, activation: ELU\n",
    "            Fully connected: neurons: 1 (output)\n",
    "\n",
    "        the convolution layers are meant to handle feature engineering\n",
    "        the fully connected layer for predicting the steering angle.\n",
    "        \"\"\"\n",
    "        super(NetworkNvidia, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 24, 5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(24, 36, 5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(36, 48, 5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(48, 64, 3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=64 * 2 * 33, out_features=100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=100, out_features=50),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=50, out_features=10),\n",
    "            nn.Linear(in_features=10, out_features=3)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        input = input.view(input.size(0), 3, HEIGHT, WIDTH)\n",
    "        output = self.conv_layers(input)\n",
    "        # print(output.shape)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.linear_layers(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "model_path = os.path.join(os.getcwd(), \"models\")\n",
    "\n",
    "!mkdir models && cd models && mkdir epochs\n",
    "\n",
    "print(model_path)\n",
    "transform = transforms.Compose(\n",
    "        [transforms.Resize((70, 320), antialias=True), transforms.ToTensor()]\n",
    "    )\n",
    "model_number = \"1.2\"\n",
    "model_file_name = f\"nuscenes_model_v{model_number}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(f\"Model Version v{model_number}\")\n",
    "    print(\"final model weights will be saved to: \" + model_path)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "    scenes = nusc.scene\n",
    "\n",
    "    train_size = int(0.8 * len(scenes))\n",
    "    val_size = len(scenes) - train_size\n",
    "    train, val = random_split(scenes, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    net = NetworkNvidia().to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        for batch in enumerate(train_loader):\n",
    "            for scene in batch[1]['token']:\n",
    "                scene = nusc.get('scene', scene)\n",
    "                print(\"Training on scene \" + scene['name'])\n",
    "                scene_number = int(scene['name'].split(\"-\")[1])\n",
    "\n",
    "                if scene_number in nusc_can.can_blacklist:\n",
    "                    print(\"Skipping scene \" + str(scene_number))\n",
    "                    continue\n",
    "                \n",
    "                first_sample_token = scene['first_sample_token']\n",
    "\n",
    "                current_sample = nusc.get('sample', first_sample_token)\n",
    "\n",
    "                scene_vehicle_monitor = nusc_can.get_messages(scene['name'], 'vehicle_monitor')\n",
    "\n",
    "                while True:\n",
    "                    sensor = \"CAM_FRONT\"\n",
    "                    cam_front_data = nusc.get(\"sample_data\", current_sample[\"data\"][sensor])\n",
    "                    current_image_path = PATH + \"/\" + cam_front_data[\"filename\"]\n",
    "                    img = Image.open(current_image_path)\n",
    "\n",
    "                    img_input = transform(img).to(device)\n",
    "                    img_input = img_input.view(1, 3, HEIGHT, WIDTH)\n",
    "                \n",
    "                    current_vehicle_can = get_closest_can(current_sample[\"timestamp\"], scene_vehicle_monitor)                    \n",
    "\n",
    "                    if current_vehicle_can == {}:\n",
    "                        if current_sample['next'] == '':\n",
    "                            break\n",
    "                        else:\n",
    "                            current_sample = nusc.get('sample', current_sample['next'])\n",
    "                            continue\n",
    "\n",
    "                    normal_vm_can = normalize_vehicle_monitor_can(current_vehicle_can)\n",
    "\n",
    "                    steering_targets = normal_vm_can['steering']\n",
    "                    throttle_targets = normal_vm_can['throttle']\n",
    "                    breaking_targets = normal_vm_can['brake']\n",
    "\n",
    "                    label = torch.FloatTensor([steering_targets, throttle_targets, breaking_targets]).to(device)              \n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = net(img_input)\n",
    "\n",
    "                    # Compute loss\n",
    "                    total_loss = criterion(outputs, label)\n",
    "\n",
    "                    # Backward pass\n",
    "                    total_loss.backward()\n",
    "\n",
    "                    # Update weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if current_sample['next'] == '':\n",
    "                        break\n",
    "                    else:\n",
    "                        current_sample = nusc.get('sample', current_sample['next'])\n",
    "\n",
    "        # Validation\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in enumerate(val_loader):\n",
    "                for scene in batch[1]['token']:\n",
    "                    scene = nusc.get('scene', scene)\n",
    "                    print(\"Validating on scene \" + scene['name'])\n",
    "\n",
    "                    scene_number = int(scene['name'].split(\"-\")[1])\n",
    "\n",
    "                    if scene_number in nusc_can.can_blacklist:\n",
    "                        print(\"Skipping scene \" + str(scene_number))\n",
    "                        continue\n",
    "\n",
    "                    first_sample_token = scene['first_sample_token']\n",
    "\n",
    "                    current_sample = nusc.get('sample', first_sample_token)\n",
    "\n",
    "                    scene_vehicle_monitor = nusc_can.get_messages(scene['name'], 'vehicle_monitor')\n",
    "\n",
    "                    while True:\n",
    "                        sensor = \"CAM_FRONT\"\n",
    "                        cam_front_data = nusc.get(\"sample_data\", current_sample[\"data\"][sensor])\n",
    "                        current_image_path = PATH + \"/\" + cam_front_data[\"filename\"]\n",
    "                        img = Image.open(current_image_path)\n",
    "\n",
    "                        img_input = transform(img).to(device)\n",
    "                        img_input = img_input.view(1, 3, 70, 320)\n",
    "\n",
    "                        current_vehicle_can = get_closest_can(current_sample[\"timestamp\"], scene_vehicle_monitor)\n",
    "\n",
    "                        if current_vehicle_can == {}:\n",
    "                            if current_sample['next'] == '':\n",
    "                                break\n",
    "                            else:\n",
    "                                current_sample = nusc.get('sample', current_sample['next'])\n",
    "                                continue\n",
    "                    \n",
    "                        normal_vm_can = normalize_vehicle_monitor_can(current_vehicle_can)\n",
    "\n",
    "                        steering_targets = normal_vm_can['steering']\n",
    "                        throttle_targets = normal_vm_can['throttle']\n",
    "                        breaking_targets = normal_vm_can['brake']\n",
    "\n",
    "                        label = torch.FloatTensor([steering_targets, throttle_targets, breaking_targets]).to(device)\n",
    "\n",
    "                        outputs = net(img_input)\n",
    "\n",
    "                        val_total_loss = criterion(outputs, label)\n",
    "\n",
    "                        if current_sample['next'] == '':\n",
    "                            break\n",
    "                        else:\n",
    "                            current_sample = nusc.get('sample', current_sample['next'])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}, Validation Loss: {val_total_loss.item():.4f}\"\n",
    "        )\n",
    "        torch.save(\n",
    "            net.state_dict(),\n",
    "            os.path.join(model_path, \"epochs\", f\"model_e{epoch+1}.pth\"),\n",
    "        )\n",
    "\n",
    "    print(\"Finished training\")\n",
    "    torch.save(net.state_dict(), os.path.join(model_path, model_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scene = nusc.scene[0]\n",
    "my_scene_vehicle_monitor_can = nusc_can.get_messages(my_scene['name'], 'vehicle_monitor')\n",
    "sensor = 'CAM_FRONT'\n",
    "cam_front_data = nusc.get('sample_data', my_sample['data'][sensor])\n",
    "current_image_path = PATH+\"/\"+cam_front_data['filename']\n",
    "img = Image.open(current_image_path)\n",
    "image_tensor = transform(img)\n",
    "image_tensor = image_tensor.view(1, 3, 70, 320)\n",
    "IPImage(filename=current_image_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(model_path, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_can = get_closest_can(my_sample['timestamp'], my_scene_vehicle_monitor_can)\n",
    "normalize_vehicle_monitor_can(closest_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetworkNvidia()\n",
    "model.load_state_dict(torch.load(model_path + \"/\" + model_file_name, map_location=torch.device('mps')))\n",
    "model.eval()\n",
    "model(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "sensor = 'CAM_FRONT'\n",
    "cam_front_data = nusc.get('sample_data', my_sample['data'][sensor])\n",
    "current_image_path = PATH+\"/\"+cam_front_data['filename']\n",
    "img = Image.open(current_image_path)\n",
    "image_tensor = transform(img)\n",
    "image_tensor = image_tensor.view(1, 3, 70, 320)\n",
    "IPImage(filename=current_image_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_can = get_closest_can(my_sample['timestamp'], my_scene_vehicle_monitor_can)\n",
    "normalize_vehicle_monitor_can(closest_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "my_sample = nusc.get('sample', my_sample['next'])\n",
    "sensor = 'CAM_FRONT'\n",
    "cam_front_data = nusc.get('sample_data', my_sample['data'][sensor])\n",
    "current_image_path = PATH+\"/\"+cam_front_data['filename']\n",
    "img = Image.open(current_image_path)\n",
    "image_tensor = transform(img)\n",
    "image_tensor = image_tensor.view(1, 3, 70, 320)\n",
    "IPImage(filename=current_image_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_can = get_closest_can(my_sample['timestamp'], my_scene_vehicle_monitor_can)\n",
    "normalize_vehicle_monitor_can(closest_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(image_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
